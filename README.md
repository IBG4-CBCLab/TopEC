# Project Organization
------------

    ├── LICENSE
    ├── Makefile           <- Makefile with commands like `make data` or `make train`
    ├── README.md          <- The top-level README for developers using this project.
    ├── configs            <- Hydra config files
    ├── data
    │   ├── processed      <- Each enzyme has ".pt"  with coordinates and atom-type for each coordinate. 
    │   │                    Also Main class and hierarchical EC numbers are stored here
    │   │                      
    │   └── raw            <- Pdb files for bindingsites
    │   └── split_csv      <- CSV files that are going to be used for training, validation and test
    │
    ├── docs               <- A default Sphinx project; see sphinx-doc.org for details
    │
    ├── notebooks          <- Jupyter notebooks. GNNExplainer with data visualization notebooks
    │
    ├── logs               <- Generated by hydra. Files are stored according to date of experiment. 
    │                                  
    │
    ├── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
    │                         generated with `pip freeze > requirements.txt`
    ├── setup.py           <- makes project pip installable (pip install -e .) so src can be imported
    ├── src                <- Source code for use in this project.
    │   ├── __init__.py    <- Makes src a Python module
    │   │
    │   ├── datamodules           <- Datamodule
    │   │    ├── components
    │   │    │   └── enzyme_dataset.py <- Create PyG dataset. Full enzymes are loaded and later 
    │   │    │                            binding site is cut according to binding site center and cutting algorithm. 
    │   │    │
    │   │    └── enzyme_datamodule  <- Reads construct dataloader which returns PyG graph data with coordinates, atom types and labels.
    │   │
    │   ├── models         <- Models to use for prediction of enzyme function
    │   │    ├── components
    │   │         └── schnet.py <- SchNet modified from pytorch-geometric
    │   │         └── dimenetpp.py <- Dimenetpp modified from pytorch-geometric
    │   │         └── ginnet.py <- ginet
    │   │         └── explainer.py <- Explainer base modified from pytorch-geometric to work with x and pos instead of just x
    │   │         └── gnnexplainer.py <- GNNExplainer base modified from pytorch-geometric to work with x and pos instead of just x
    │   │          └── resolver.py <- required for dimenet
    │   │    ├── enzyme_datamodule <- General training scripts which accepts a model either schnet, dimenet++ or GINet.
    │   │   
    │   
    ├── create_dataset.py <- script to create dataset for training from raw pdb files      
    ├── train.py <- script to run train
    ├── test.py <- script to run test

--------

# General info

Pytorch Repo for enzyme classification using binding site information for the Juelich Enzyme Prediction Voucher
Uses pytorch, pytorch-lightning, pytorch-geometric, hydra and tensorboard

Task is to classify enzymes according to EC (https://en.wikipedia.org/wiki/Enzyme_Commission_number) using only binding site information instead of full enzyme. 

Input data is .pdb file which consists information about binding site. Binding site information files are constructed using different radius such as 16, 24, 32, 48 Angstrom. 
From calculated binding site center data can be either constructed as a cube shape or sphere. 

Two main approach is implemented. In first approach, data pointclouds are converted to voxelised data and in second approach we treat each atom as a node in 3D graph.

- In first approach, 3D convolutional neural network is used. Network can be constructed with different hyperparameters. 
Model should perform well even if the given binding site rotated since rotating the binding site does not affects its EC number. In first approach, rotation and translation augmentation is used to train model to generalize well.

- In second approach, each atom is treated as a node in 3D graph. Each binding site is a 3D graph and processed using different models (SchNet and DimeNet++). 
As augmentation, random translation(from pytroch geometric) of each atom is applied.
	
# Setup
Install pytorch-geometric following instructions below:

https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html
Install

Run
```
$ python -m pip install -e .
```
# Running

Place pdb files for corresponding radius of binding-site under data/raw and enzyme_data.csv to src/data/raw

Raw files are formatted as {EnzymeName}/{EnzymeName}_{binding_site_num}.pdb 

binding_site_num: Indicates the ranking of the binding site. 1 is the more probable binding site predicted for a given enzyme

Create .pt file for each binding site (coordinates and atom-type for each coordinate) using (src/data/prepare_data.py)

- Creates train, validation and test datasets. Temporal information is used to divide dataset. 
- Validation dataset contains enzymes that are processed after enzymes in train dataset and test datset contains enzymes that are processed after validation dataset.

Experiments are run using hydra configs. Each part of training has its own configs in different folder (callbacks, datamodule, experiment, logger, trainer). 

Train using models from 3D convolutions, SchNet and Dimenet++ using run_training.py

All configurations are under configs/ folder. Experiments/ folder under configs/ has experiment yaml for each different model.